{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Shoreline extraction at the benchmark sites with CoastSat\n",
    "\n",
    "\n",
    "This notebook shows how to extract time-series of shoreline change at the 4 benchmark sites using the CoastSat toolbox.\n",
    "\n",
    "## Initial settings\n",
    "\n",
    "Refer to the **Installation** section of the [CoastSat README](https://github.com/kvos/CoastSat) for instructions on how to install the Python packages necessary to run the software, including Google Earth Engine Python API. If that step has been completed correctly, the following packages should be imported without any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "Benchmark datasets available:\n",
      "['CALAMILLOR', 'DUCK', 'NARRABEEN', 'TORREYPINES', 'TRUCVERT']\n",
      "NARRABEEN: {'beach_slope': 0.1, 'contour_level': 0.7, 'epsg': 28356}\n",
      "DUCK: {'beach_slope': 0.1, 'contour_level': 0.585, 'epsg': 32119}\n",
      "TRUCVERT: {'beach_slope': 0.05, 'contour_level': 1.5, 'epsg': 32630}\n",
      "TORREYPINES: {'beach_slope': 0.045, 'contour_level': 0.792, 'epsg': 26946}\n",
      "CALAMILLOR: {'beach_slope': 0.1, 'contour_level': 0, 'epsg': 2062}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.ion()\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import interpolate\n",
    "from scipy import stats\n",
    "import pytz\n",
    "import json \n",
    "# load coastsat package located under algorithms/COASTSAT/\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects\n",
    "\n",
    "# filepaths to all the datasets\n",
    "fp_datasets = os.path.join(os.path.join(os.pardir,os.pardir,'datasets'))\n",
    "names_datasets = os.listdir(fp_datasets)\n",
    "names_datasets = [_ for _ in names_datasets if _ not in ['README.md','sites_info.txt']]\n",
    "print('\\nBenchmark datasets available:\\n%s'%(names_datasets))\n",
    "                \n",
    "# load site info dict if exists or create\n",
    "fp_info = os.path.join(fp_datasets,'sites_info.txt')\n",
    "if os.path.exists(fp_info):\n",
    "    with open(fp_info,'r') as f: sites_info = json.load(f)  \n",
    "    print('\\nLoaded sites_info.txt.')\n",
    "else:\n",
    "    sites_info = {'NARRABEEN':{'beach_slope':0.1,'contour_level':0.7,'epsg':28356},\n",
    "                  'DUCK':{'beach_slope':0.1,'contour_level':0.585,'epsg':32119},\n",
    "                  'TRUCVERT':{'beach_slope':0.05,'contour_level':1.5,'epsg':32630},\n",
    "                  'TORREYPINES':{'beach_slope':0.045,'contour_level':0.792,'epsg':26946},\n",
    "                  'CALAMILLOR':{'beach_slope':0.1,'contour_level':0,'epsg':2062},\n",
    "                 }\n",
    "    with open(fp_info,'w') as f: json.dump(sites_info,f,indent=4)\n",
    "for key in sites_info.keys(): print('%s: %s'%(key,sites_info[key]))\n",
    "\n",
    "# folder to save the images in\n",
    "fp_images = os.path.join(os.getcwd(),'satellite_data_C02')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Retrieval of the images from GEE for the test sites\n",
    "\n",
    "Here we download all the Landsat (5,7,8,9) amd Sentinel-2 images between 1984 and 2022 for the benchmark sites using the provided ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALAMILLOR\n",
      "images already downloaded\n",
      "DUCK\n",
      "images already downloaded\n",
      "NARRABEEN\n",
      "images already downloaded\n",
      "TORREYPINES\n",
      "images already downloaded\n",
      "TRUCVERT\n",
      "images already downloaded\n"
     ]
    }
   ],
   "source": [
    "# satellites to include\n",
    "sat_list = ['L5','L7','L8']\n",
    "# choose Landsat collection 'C01' or 'C02'\n",
    "landsat_collection = 'C02'\n",
    "# dates\n",
    "dates = ['1984-01-01', '2022-01-01']\n",
    "# loop over each site\n",
    "for sitename in names_datasets:\n",
    "    print(sitename)\n",
    "    data_folder = os.path.join(fp_datasets,sitename)\n",
    "    # load polygon ROI\n",
    "    fn_polygon = os.path.join(data_folder, '%s_polygon.geojson'%sitename)\n",
    "    gdf_polygon = gpd.read_file(fn_polygon)\n",
    "    polygon = np.array(gdf_polygon.loc[0,'geometry'].exterior.coords)\n",
    "    # create inputs dictionary\n",
    "    inputs = {}\n",
    "    inputs['sitename'] = sitename\n",
    "    inputs['polygon'] = [[[_[0], _[1]] for _ in polygon]]\n",
    "    inputs['sat_list'] = sat_list\n",
    "    inputs['dates'] = dates\n",
    "    inputs['landsat_collection'] = landsat_collection\n",
    "    inputs['filepath'] = fp_images\n",
    "    if os.path.exists(os.path.join(fp_images,sitename,sitename+'_metadata.pkl')): \n",
    "        print('images already downloaded')\n",
    "        continue\n",
    "    # download the imagery\n",
    "    metadata = SDS_download.retrieve_images(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Extraction of shoreline time-series along the transects\n",
    "\n",
    "In this cell we map the position of the shoreline on the satellite images, compute the intersections between the shorelines and the transects and perform a tidal correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CALAMILLOR\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:2062\n",
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   100%\n",
      "L8:   100%\n",
      "523 duplicates\n",
      "109 bad georef\n",
      "0 / 487 images different size\n",
      "12 / 487 images wrong classif\n",
      "2% images removed\n",
      "\n",
      "Kept images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\CALAMILLOR\\jpg_files\\all_images\n",
      "\n",
      "Rejected images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\CALAMILLOR\\jpg_files\\rejected\n",
      "Mean clasification image was saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\CALAMILLOR\\classif_qa.jpg\n",
      "\n",
      "Processing DUCK\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:32119\n",
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   100%\n",
      "L8:   100%\n",
      "0 duplicates\n",
      "6 bad georef\n",
      "0 / 708 images different size\n",
      "13 / 708 images wrong classif\n",
      "1% images removed\n",
      "\n",
      "Kept images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\DUCK\\jpg_files\\all_images\n",
      "\n",
      "Rejected images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\DUCK\\jpg_files\\rejected\n",
      "Mean clasification image was saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\DUCK\\classif_qa.jpg\n",
      "Loaded transects in epsg:4326...converted to epsg:32119\n",
      "Extracting closest points: 100%\n",
      "Removing outliers...\n",
      "\n",
      "Processing NARRABEEN\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:28356\n",
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   100%\n",
      "L8:   100%\n",
      "89 duplicates\n",
      "0 bad georef\n",
      "0 / 545 images different size\n",
      "6 / 545 images wrong classif\n",
      "1% images removed\n",
      "\n",
      "Kept images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\NARRABEEN\\jpg_files\\all_images\n",
      "\n",
      "Rejected images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\NARRABEEN\\jpg_files\\rejected\n",
      "Mean clasification image was saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\NARRABEEN\\classif_qa.jpg\n",
      "Loaded transects in epsg:4326...converted to epsg:28356\n",
      "Extracting closest points: 100%\n",
      "Removing outliers...\n",
      "\n",
      "Processing TORREYPINES\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:26946\n",
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   100%\n",
      "L8:   100%\n",
      "0 duplicates\n",
      "0 bad georef\n",
      "0 / 600 images different size\n",
      "4 / 600 images wrong classif\n",
      "0% images removed\n",
      "\n",
      "Kept images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\TORREYPINES\\jpg_files\\all_images\n",
      "\n",
      "Rejected images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\TORREYPINES\\jpg_files\\rejected\n",
      "Mean clasification image was saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\TORREYPINES\\classif_qa.jpg\n",
      "Loaded transects in epsg:4326...converted to epsg:26946\n",
      "Extracting closest points: 100%\n",
      "Removing outliers...\n",
      "***\n",
      "Processing TRUCVERT\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:32630\n",
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   100%\n",
      "L8:   100%\n",
      "0 duplicates\n",
      "160 bad georef\n",
      "0 / 608 images different size\n",
      "3 / 608 images wrong classif\n",
      "0% images removed\n",
      "\n",
      "Kept images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\TRUCVERT\\jpg_files\\all_images\n",
      "\n",
      "Rejected images were saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\TRUCVERT\\jpg_files\\rejected\n",
      "Mean clasification image was saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data_C02\\TRUCVERT\\classif_qa.jpg\n",
      "Loaded transects in epsg:4326...converted to epsg:32630\n",
      "Extracting closest points: 100%\n",
      "Removing outliers...\n",
      "*******************************************************"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "output_folder = 'tidally_corrected_timeseries_MHWS'\n",
    "# extract shoreline time-series at each site\n",
    "inputs = {}\n",
    "inputs['filepath'] =  fp_images\n",
    "inputs['landsat_collection'] = 'C02'\n",
    "inputs['dates'] =  ['1984-01-01', '2022-01-01']\n",
    "inputs['sat_list'] = ['L5','L7','L8']\n",
    "for sitename in names_datasets:\n",
    "    print('\\nProcessing %s'%sitename)\n",
    "    inputs['sitename'] = sitename\n",
    "    #########################################################################################################\n",
    "    # Map shorelines\n",
    "    #########################################################################################################\n",
    "    # load metadata\n",
    "    metadata = SDS_download.get_metadata(inputs)\n",
    "    # remove S2 in not in the inputs\n",
    "    if ('S2' in metadata.keys()) and ('S2' not in inputs['sat_list']):  metadata.pop('S2');\n",
    "    # load settings file\n",
    "    fp_settings = 'settings_shoreline_mapping.txt'\n",
    "    with open(fp_settings,'r') as f: settings = json.load(f)\n",
    "    settings['inputs'] = inputs\n",
    "    # take epsg from sites_info \n",
    "    settings['output_epsg'] = sites_info[sitename]['epsg']\n",
    "    # load reference shoreline\n",
    "    data_folder = os.path.join(fp_datasets,sitename)\n",
    "    fn_refsl = os.path.join(data_folder, '%s_reference_shoreline.geojson'%sitename)\n",
    "    gdf_refsl = gpd.read_file(fn_refsl)\n",
    "    print('Loaded reference shoreline in epsg:%d'%gdf_refsl.crs.to_epsg(), end='...')\n",
    "    gdf_refsl.to_crs(epsg=settings['output_epsg'], inplace=True)\n",
    "    print('converted to epsg:%d'%gdf_refsl.crs.to_epsg())\n",
    "    refsl = np.array(gdf_refsl.loc[0,'geometry'].coords)\n",
    "    settings['reference_shoreline'] = refsl\n",
    "    # extract shorelines from all images (creates or loads output.pkl)\n",
    "    fn_output = os.path.join(inputs['filepath'], sitename, sitename + '_output' + '.pkl')\n",
    "    if os.path.exists(fn_output):\n",
    "        with open(fn_output, 'rb') as f:\n",
    "            output = pickle.load(f)\n",
    "        print('loaded existing file at %s'%fn_output)\n",
    "    else:\n",
    "        %matplotlib qt\n",
    "        settings['save_figure'] = True\n",
    "        output = SDS_shoreline.extract_shorelines(metadata, settings)\n",
    "        # remove duplicates (images taken on the same date by the same satellite)\n",
    "        output = SDS_tools.remove_duplicates(output)\n",
    "        # remove inaccurate georeferencing (set threshold to 10 m)\n",
    "        output = SDS_tools.remove_inaccurate_georef(output, 10)\n",
    "        # remove bad images based on average classification (only works for same resolution, Landsat)\n",
    "        if not 'S2' in metadata.keys():\n",
    "            # settings for discarding the badly classified images\"\n",
    "            settings['prc_pixel'] = 0.15    # minimum percentage of change from land to water for defining the wet/dry areas\n",
    "            settings['prc_image'] = 0.3     # maximum percentage of misclassified pixels allowed in the image, otherwise discarded\n",
    "            output = SDS_shoreline.remove_bad_images(output,settings)\n",
    "        else:\n",
    "            output.pop('im_classif')\n",
    "            with open(os.path.join(inputs['filepath'], inputs['sitename'], sitename + '_output' + '.pkl'), 'wb') as f:\n",
    "                pickle.dump(output,f)\n",
    "    if sitename == 'CALAMILLOR': continue    \n",
    "    # load transects\n",
    "    fn_transects = os.path.join(data_folder, '%s_transects.geojson'%sitename)\n",
    "    gdf_transects = gpd.read_file(fn_transects)\n",
    "    print('Loaded transects in epsg:%d'%gdf_transects.crs.to_epsg(), end='...')\n",
    "    gdf_transects.to_crs(epsg=settings['output_epsg'], inplace=True)\n",
    "    print('converted to epsg:%d'%gdf_transects.crs.to_epsg())\n",
    "    # put transects into a dictionary with their name\n",
    "    transects = dict([])\n",
    "    for i in gdf_transects.index:\n",
    "        transects[gdf_transects.loc[i,'name']] = np.array(gdf_transects.loc[i,'geometry'].coords)\n",
    "    fig,ax = plt.subplots(1,1,figsize=[15,8], tight_layout=True)\n",
    "    ax.axis('equal')\n",
    "    ax.set(xlabel='Eastings',ylabel='Northings',title='%s - satellite-derived shorelines and transect locations'%sitename)\n",
    "    ax.grid(linestyle=':', color='0.5')\n",
    "    for i in range(len(output['shorelines'])):\n",
    "        sl = output['shorelines'][i]\n",
    "        date = output['dates'][i]\n",
    "        ax.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "    for i,key in enumerate(list(transects.keys())):\n",
    "        ax.plot(transects[key][0,0],transects[key][0,1], 'bo', ms=5)\n",
    "        ax.plot(transects[key][:,0],transects[key][:,1],'k-',lw=1)\n",
    "        if len(transects) < 50: ax.text(transects[key][0,0]-100, transects[key][0,1]+100, key,va='center', ha='right')\n",
    "    fig.savefig(os.path.join(inputs['filepath'], inputs['sitename'], 'mapped_shorelines.jpg'),dpi=200)\n",
    "    #########################################################################################################\n",
    "    # Compute intersections\n",
    "    #########################################################################################################\n",
    "    # load settings file\n",
    "    fp_settings = 'settings_transect_intersections.txt'\n",
    "    with open(fp_settings,'r') as f: settings_transects = json.load(f)\n",
    "    cross_distance = SDS_transects.compute_intersection_QC(output, transects, settings_transects)\n",
    "    #########################################################################################################\n",
    "    # Tidal correction\n",
    "    #########################################################################################################\n",
    "    # load tide time-series\n",
    "    fn_tides = os.path.join(data_folder,'%s_tides.csv'%sitename)\n",
    "    tide_data = pd.read_csv(fn_tides, parse_dates=['dates'])\n",
    "    dates_ts = [pytz.utc.localize(_.to_pydatetime()) for _ in tide_data['dates']]\n",
    "    tides_ts = np.array(tide_data['tides'])\n",
    "    # get tide levels corresponding to the time of image acquisition\n",
    "    dates_sat = output['dates']\n",
    "    tides_sat = SDS_tools.get_closest_datapoint(dates_sat, dates_ts, tides_ts)\n",
    "    # plot the subsampled tide data\n",
    "    fig, ax = plt.subplots(1,1,figsize=(15,4), tight_layout=True)\n",
    "    ax.grid(which='major', linestyle=':', color='0.5')\n",
    "    ax.plot(tide_data['dates'], tide_data['tides'], '-', color='0.6', label='all time-series')\n",
    "    ax.plot(dates_sat, tides_sat, '-o', color='k', ms=6, mfc='w',lw=1, label='image acquisition')\n",
    "    ax.set(ylabel='tide level [m]',xlim=[dates_sat[0],dates_sat[-1]], title='%s - water levels at the time of image acquisition'%sitename)\n",
    "    ax.legend()\n",
    "    fig.savefig(os.path.join(inputs['filepath'], inputs['sitename'], 'tide_timeseries.jpg'),dpi=200)\n",
    "    # tidal correction along each transect\n",
    "    reference_elevation = sites_info[sitename]['contour_level'] # elevation at which you would like the shoreline time-series to be\n",
    "    beach_slope = sites_info[sitename]['beach_slope']           # beach slope, uniform for all transects\n",
    "    cross_distance_tidally_corrected, tidal_corrections = {},{}\n",
    "    for key in cross_distance.keys():\n",
    "        correction = (tides_sat-reference_elevation)/beach_slope\n",
    "        tidal_corrections[key] = correction\n",
    "        cross_distance_tidally_corrected[key] = cross_distance[key] + correction\n",
    "    if sitename == 'TRUCVERT':     # remove low tide images for TRUCVERT only (based on Castelle et al. 2021)\n",
    "        for key in cross_distance_tidally_corrected.keys():\n",
    "            for i in range(len(cross_distance_tidally_corrected[key])):\n",
    "                if tides_sat[i] < 0.2: cross_distance_tidally_corrected[key][i] = np.nan\n",
    "    # remove outliers\n",
    "    print('\\nRemoving outliers...')\n",
    "    cross_distance_tidally_corrected = SDS_transects.reject_outliers(cross_distance_tidally_corrected,output,settings_transects)\n",
    "    # save in .csv files\n",
    "    fp_raw_timeseries = os.path.join(inputs['filepath'], inputs['sitename'], 'raw_timeseries')\n",
    "    fp_tc_timeseries = os.path.join(inputs['filepath'], inputs['sitename'], output_folder)\n",
    "    if not os.path.exists(fp_raw_timeseries): os.makedirs(fp_raw_timeseries)\n",
    "    if not os.path.exists(fp_tc_timeseries): os.makedirs(fp_tc_timeseries)\n",
    "    for key in cross_distance_tidally_corrected.keys():\n",
    "        out_dict = dict([])\n",
    "        out_dict['dates'] = output['dates']\n",
    "        out_dict[key] = cross_distance_tidally_corrected[key]\n",
    "        out_dict['satname'] = output['satname']\n",
    "        df = pd.DataFrame(out_dict)\n",
    "        df.index=df['dates']\n",
    "        df.pop('dates')\n",
    "        # save tidally_corrected timeseries\n",
    "        fn = os.path.join(fp_tc_timeseries,'%s_timeseries_tidally_corrected.csv'%key)\n",
    "        df.to_csv(fn, sep=',')\n",
    "        # save raw timeseries\n",
    "        out_dict[key] = cross_distance_tidally_corrected[key] - tidal_corrections[key]\n",
    "        df = pd.DataFrame(out_dict)\n",
    "        df.index=df['dates']\n",
    "        df.pop('dates')\n",
    "        fn = os.path.join(fp_raw_timeseries,'%s_timeseries_raw.csv'%key)\n",
    "        df.to_csv(fn, sep=',')\n",
    "    # plot time-series\n",
    "    fp_plots = os.path.join(inputs['filepath'], inputs['sitename'], 'plots')\n",
    "    if not os.path.exists(fp_plots): os.makedirs(fp_plots)\n",
    "    month_colors = plt.cm.get_cmap('tab20')\n",
    "    for key in cross_distance_tidally_corrected.keys():\n",
    "        chainage = cross_distance_tidally_corrected[key]\n",
    "        # remove nans\n",
    "        idx_nan = np.isnan(chainage)\n",
    "        dates_nonan = [dates_sat[_] for _ in np.where(~idx_nan)[0]]\n",
    "        chainage = chainage[~idx_nan] \n",
    "        # compute shoreline monthly averages\n",
    "        dict_month, dates_month, chainage_month, list_month = SDS_transects.monthly_average(dates_nonan, chainage)\n",
    "        # plot monthly averages\n",
    "        fig,ax=plt.subplots(1,1,figsize=[14,4],tight_layout=True)\n",
    "        ax.grid(b=True,which='major', linestyle=':', color='0.5')\n",
    "        ax.set_title('Time-series at %s'%key, x=0, ha='left')\n",
    "        ax.set(ylabel='distance [m]')\n",
    "        ax.plot(dates_nonan, chainage,'+', lw=1, color='k', mfc='w', ms=4, alpha=0.5,label='raw datapoints')\n",
    "        ax.plot(dates_month, chainage_month, '-', lw=2, color='k', mfc='w', ms=4, label='monthly-averaged')\n",
    "        # for k,month in enumerate(dict_month.keys()):\n",
    "        #     ax.plot(dict_month[month]['dates'], dict_month[month]['chainages'],\n",
    "        #              'o', mec='k', color=month_colors(k), label=month,ms=5)\n",
    "        ax.legend(loc='lower left',ncol=7,markerscale=1.5,frameon=True,edgecolor='k',columnspacing=1)\n",
    "        fig.savefig(os.path.join(fp_plots,'%s_timeseries.jpg'%key),dpi=200)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time-series of shoreline change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'tidally_corrected_timeseries_MHWS'\n",
    "for sitename in names_datasets:\n",
    "    if sitename == 'CALAMILLOR': continue \n",
    "    inputs['sitename'] = sitename\n",
    "    # load time-series\n",
    "    fp_tc_timeseries = os.path.join(inputs['filepath'], inputs['sitename'], output_folder)\n",
    "    fn_transects = os.listdir(fp_tc_timeseries)\n",
    "    if sitename == 'DUCK': transects = ['-91','1','1006','1097']\n",
    "    elif sitename == 'TRUCVERT': transects = ['-400','-300','-200','-100']\n",
    "    elif sitename == 'TORREYPINES': transects =  ['PF525','PF535','PF585','PF595']\n",
    "    elif sitename == 'NARRABEEN': transects = [_.split('_')[0] for _ in fn_transects]\n",
    "    # make figure\n",
    "    fig = plt.figure(figsize=[14,8], tight_layout=True)\n",
    "    fig.suptitle('Time-series of shoreline change at %s ( %.2fm contour)'%(sitename,sites_info[sitename]['contour_level']))\n",
    "    gs = gridspec.GridSpec(len(transects),1)\n",
    "    gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.12)\n",
    "    for i,key in enumerate(transects):\n",
    "        fn = os.path.join(fp_tc_timeseries,'%s_timeseries_tidally_corrected.csv'%key)\n",
    "        df = pd.read_csv(fn,sep=',',parse_dates=['dates'])  \n",
    "        chainage = np.array(df[key])\n",
    "        dates_sat = [_.to_pydatetime() for _ in df['dates']]\n",
    "        # remove nans\n",
    "        idx_nan = np.isnan(chainage)\n",
    "        dates_nonan = [dates_sat[_] for _ in np.where(~idx_nan)[0]]\n",
    "        chainage = chainage[~idx_nan]\n",
    "        # compute shoreline monthly averages\n",
    "        dict_month, dates_month, chainage_month, list_month = SDS_transects.monthly_average(dates_nonan, chainage)\n",
    "        # plot time-series\n",
    "        ax = fig.add_subplot(gs[i,0])\n",
    "        ax.grid(b=True,linestyle=':', color='0.5')\n",
    "        ax.plot(dates_nonan, chainage,'+', lw=1, color='C0', mfc='w', ms=4, alpha=0.5,label='raw datapoints')\n",
    "        ax.plot(dates_month, chainage_month, '-', lw=1.5, color='C0', mfc='w', ms=4, label='monthly-averaged')\n",
    "        ax.set_ylabel('distance [m]', fontsize=12)\n",
    "        ax.text(0.1,0.95, key, bbox=dict(boxstyle=\"square\", ec='k',fc='w'), ha='center',\n",
    "                va='top', transform=ax.transAxes, fontsize=14)  \n",
    "    fig.savefig(os.path.join(inputs['filepath'], inputs['sitename'],'%s_timeseries.jpg'%sitename), dpi=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
