{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Shoreline extraction at the benchmark sites with CoastSat\n",
    "\n",
    "\n",
    "This notebook shows how to extract time-series of shoreline change at the 4 benchmark sites using the CoastSat toolbox.\n",
    "\n",
    "## Initial settings\n",
    "\n",
    "Refer to the **Installation** section of the [CoastSat README](https://github.com/kvos/CoastSat) for instructions on how to install the Python packages necessary to run the software, including Google Earth Engine Python API. If that step has been completed correctly, the following packages should be imported without any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark datasets available:\n",
      "['DUCK', 'NARRABEEN', 'TORREYPINES', 'TRUCVERT']\n",
      "\n",
      "Loaded sites_info.txt.\n",
      "NARRABEEN: {'beach_slope': 0.1, 'contour_level': 0.585, 'epsg': 28356}\n",
      "DUCK: {'beach_slope': 0.1, 'contour_level': 0.7, 'epsg': 32119}\n",
      "TRUCVERT: {'beach_slope': 0.05, 'contour_level': 1.5, 'epsg': 32630}\n",
      "TORREYPINES: {'beach_slope': 0.045, 'contour_level': 0.792, 'epsg': 26946}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.ion()\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import interpolate\n",
    "from scipy import stats\n",
    "import pytz\n",
    "import json \n",
    "# load coastsat package located under algorithms/COASTSAT/\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects\n",
    "\n",
    "# filepaths to all the datasets\n",
    "fp_datasets = os.path.join(os.path.join(os.pardir,os.pardir,'datasets'))\n",
    "names_datasets = os.listdir(fp_datasets)\n",
    "names_datasets = [_ for _ in names_datasets if _ not in ['README.md','sites_info.txt']]\n",
    "print('\\nBenchmark datasets available:\\n%s'%(names_datasets))\n",
    "                \n",
    "# load site info dict if exists or create\n",
    "fp_info = os.path.join(fp_datasets,'sites_info.txt')\n",
    "if os.path.exists(fp_info):\n",
    "    with open(fp_info,'r') as f: sites_info = json.load(f)  \n",
    "    print('\\nLoaded sites_info.txt.')\n",
    "else:\n",
    "    sites_info = {'NARRABEEN':{'beach_slope':0.1,'contour_level':0.585,'epsg':28356},\n",
    "                  'DUCK':{'beach_slope':0.1,'contour_level':0.7,'epsg':32119},\n",
    "                  'TRUCVERT':{'beach_slope':0.05,'contour_level':1.5,'epsg':32630},\n",
    "                  'TORREYPINES':{'beach_slope':0.045,'contour_level':0.792,'epsg':26946},\n",
    "                 }\n",
    "    with open(fp_info,'w') as f: json.dump(sites_info,f,indent=4)\n",
    "for key in sites_info.keys(): print('%s: %s'%(key,sites_info[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Retrieval of the images from GEE for the test sites\n",
    "\n",
    "Here we download all the Landsat (5,7,8,9) amd Sentinel-2 images between 1984 and 2022 for the benchmark sites using the provided ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUCK\n",
      "NARRABEEN\n",
      "TORREYPINES\n",
      "TRUCVERT\n"
     ]
    }
   ],
   "source": [
    "# satellites to include\n",
    "sat_list = ['L5','L7','L8','S2']\n",
    "# choose Landsat collection 'C01' or 'C02'\n",
    "landsat_collection = 'C01'\n",
    "# dates\n",
    "dates = ['1984-01-01', '2022-01-01']\n",
    "# folder to save the images in\n",
    "fp_images = os.path.join(os.getcwd(),'satellite_data')\n",
    "# loop over each site\n",
    "for sitename in names_datasets:\n",
    "    print(sitename)\n",
    "    data_folder = os.path.join(fp_datasets,sitename)\n",
    "    # load polygon ROI\n",
    "    fn_polygon = os.path.join(data_folder, '%s_polygon.geojson'%sitename)\n",
    "    gdf_polygon = gpd.read_file(fn_polygon)\n",
    "    polygon = np.array(gdf_polygon.loc[0,'geometry'].exterior.coords)\n",
    "    # create inputs dictionary\n",
    "    inputs = {}\n",
    "    inputs['sitename'] = sitename\n",
    "    inputs['polygon'] = [[[_[0], _[1]] for _ in polygon]]\n",
    "    inputs['sat_list'] = sat_list\n",
    "    inputs['dates'] = dates\n",
    "    inputs['landsat_collection'] = landsat_collection\n",
    "    inputs['filepath'] = fp_images\n",
    "    if os.path.exists(os.path.join(fp_images,sitename,sitename+'_metadata.pkl')): continue\n",
    "    # download the imagery\n",
    "    metadata = SDS_download.retrieve_images(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Extraction of shoreline time-series along the transects\n",
    "\n",
    "In this cell we map the position of the shoreline on the satellite images, compute the intersections between the shorelines and the transects and perform a tidal correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DUCK\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:32119\n",
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   100%\n",
      "L8:   100%\n",
      "0 duplicates\n",
      "10 bad georef\n",
      "0 / 641 images different size\n",
      "6 / 641 images wrong classif\n",
      "0% images removed\n",
      "Mean clasification image was saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data\\DUCK\\classif_qa.jpg\n",
      "Loaded transects in epsg:4326\n",
      "Converted to epsg:32119\n",
      "Extracting closest points: 100%removing outliers...\n",
      "Processing NARRABEEN\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:28356\n",
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   100%\n",
      "L8:   100%\n",
      "89 duplicates\n",
      "0 bad georef\n",
      "0 / 545 images different size\n",
      "5 / 545 images wrong classif\n",
      "0% images removed\n",
      "Mean clasification image was saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data\\NARRABEEN\\classif_qa.jpg\n",
      "Loaded transects in epsg:4326\n",
      "Converted to epsg:28356\n",
      "Extracting closest points: 100%removing outliers...\n",
      "Processing TORREYPINES\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:26946\n",
      "Mapping shorelines:\n",
      "L5:   100%\n",
      "L7:   100%\n",
      "L8:   100%\n",
      "0 duplicates\n",
      "0 bad georef\n",
      "0 / 604 images different size\n",
      "3 / 604 images wrong classif\n",
      "0% images removed\n",
      "Mean clasification image was saved in C:\\Users\\z5030440\\Documents\\SDS_Benchmark\\algorithms\\COASTSAT\\satellite_data\\TORREYPINES\\classif_qa.jpg\n",
      "Loaded transects in epsg:4326\n",
      "Converted to epsg:26946\n",
      "Extracting closest points: 100%removing outliers...\n",
      "****Processing TRUCVERT\n",
      "Loaded reference shoreline in epsg:4326...converted to epsg:32630\n",
      "Mapping shorelines:\n",
      "L5:   51%"
     ]
    }
   ],
   "source": [
    "# extract shoreline time-series at each site\n",
    "inputs = {}\n",
    "inputs['filepath'] =  os.path.join(os.getcwd(),'satellite_data')\n",
    "inputs['landsat_collection'] = 'C01'\n",
    "inputs['dates'] =  ['1984-01-01', '2022-01-01']\n",
    "inputs['sat_list'] = ['L5','L7','L8']\n",
    "for sitename in names_datasets:\n",
    "    print('Processing %s'%sitename)\n",
    "    inputs['sitename'] = sitename\n",
    "    #########################################################################################################\n",
    "    # Map shorelines\n",
    "    #########################################################################################################\n",
    "    # load metadata\n",
    "    metadata = SDS_download.get_metadata(inputs)\n",
    "    # remove S2 in not in the inputs\n",
    "    if 'S2' not in inputs['sat_list']:  metadata.pop('S2');\n",
    "    # load settings file\n",
    "    fp_settings = 'settings_shoreline_mapping.txt'\n",
    "    with open(fp_settings,'r') as f: settings = json.load(f)\n",
    "    settings['inputs'] = inputs\n",
    "    # take epsg from sites_info \n",
    "    settings['output_epsg'] = sites_info[sitename]['epsg']\n",
    "    # load reference shoreline\n",
    "    data_folder = os.path.join(fp_datasets,sitename)\n",
    "    fn_refsl = os.path.join(data_folder, '%s_reference_shoreline.geojson'%sitename)\n",
    "    gdf_refsl = gpd.read_file(fn_refsl)\n",
    "    print('Loaded reference shoreline in epsg:%d'%gdf_refsl.crs.to_epsg(), end='...')\n",
    "    gdf_refsl.to_crs(epsg=settings['output_epsg'], inplace=True)\n",
    "    print('converted to epsg:%d'%gdf_refsl.crs.to_epsg())\n",
    "    refsl = np.array(gdf_refsl.loc[0,'geometry'].coords)\n",
    "    settings['reference_shoreline'] = refsl\n",
    "    # extract shorelines from all images (creates or loads output.pkl)\n",
    "    fn_output = os.path.join(inputs['filepath'], sitename, sitename + '_output' + '.pkl')\n",
    "    if os.path.exists(fn_output):\n",
    "        with open(fn_output, 'rb') as f:\n",
    "            output = pickle.load(f)\n",
    "        print('loaded existing file at %s'%fn_output)\n",
    "    else:\n",
    "        %matplotlib qt\n",
    "        output = SDS_shoreline.extract_shorelines(metadata, settings)\n",
    "        # remove duplicates (images taken on the same date by the same satellite)\n",
    "        output = SDS_tools.remove_duplicates(output)\n",
    "        # remove inaccurate georeferencing (set threshold to 10 m)\n",
    "        output = SDS_tools.remove_inaccurate_georef(output, 10)\n",
    "        # remove bad images based on average classification (only works for same resolution, Landsat)\n",
    "        if not 'S2' in metadata.keys():\n",
    "            # settings for discarding the badly classified images\"\n",
    "            settings['prc_pixel'] = 0.15    # minimum percentage of change from land to water for defining the wet/dry areas\n",
    "            settings['prc_image'] = 0.3     # maximum percentage of misclassified pixels allowed in the image, otherwise discarded\n",
    "            output = SDS_shoreline.remove_bad_images(output,settings)\n",
    "        else:\n",
    "            output.pop('im_classif')\n",
    "            with open(os.path.join(filepath, sitename + '_output' + '.pkl'), 'wb') as f:\n",
    "                pickle.dump(output,f)\n",
    "    # load transects\n",
    "    fn_transects = os.path.join(data_folder, '%s_transects.geojson'%sitename)\n",
    "    gdf_transects = gpd.read_file(fn_transects)\n",
    "    print('Loaded transects in epsg:%d'%gdf_transects.crs.to_epsg())\n",
    "    gdf_transects.to_crs(epsg=settings['output_epsg'], inplace=True)\n",
    "    print('Converted to epsg:%d'%gdf_transects.crs.to_epsg())\n",
    "    # put transects into a dictionary with their name\n",
    "    transects = dict([])\n",
    "    for i in gdf_transects.index:\n",
    "        transects[gdf_transects.loc[i,'name']] = np.array(gdf_transects.loc[i,'geometry'].coords)\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('Eastings')\n",
    "    plt.ylabel('Northings')\n",
    "    plt.grid(linestyle=':', color='0.5')\n",
    "    for i in range(len(output['shorelines'])):\n",
    "        sl = output['shorelines'][i]\n",
    "        date = output['dates'][i]\n",
    "        plt.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "    for i,key in enumerate(list(transects.keys())):\n",
    "        plt.plot(transects[key][0,0],transects[key][0,1], 'bo', ms=5)\n",
    "        plt.plot(transects[key][:,0],transects[key][:,1],'k-',lw=1)\n",
    "        plt.text(transects[key][0,0]-100, transects[key][0,1]+100, key,\n",
    "                    va='center', ha='right', bbox=dict(boxstyle=\"square\", ec='k',fc='w'))\n",
    "    fig.savefig(os.path.join(inputs['filepath'], inputs['sitename'], 'mapped_shorelines.jpg'),dpi=200)\n",
    "    #########################################################################################################\n",
    "    # Compute intersections\n",
    "    #########################################################################################################\n",
    "    # load settings file\n",
    "    fp_settings = 'settings_transect_intersections.txt'\n",
    "    with open(fp_settings,'r') as f: settings_transects = json.load(f)\n",
    "    cross_distance = SDS_transects.compute_intersection_QC(output, transects, settings_transects)\n",
    "    #########################################################################################################\n",
    "    # Tidal correction\n",
    "    #########################################################################################################\n",
    "    # load tide time-series\n",
    "    fn_tides = os.path.join(data_folder,'%s_tides.csv'%sitename)\n",
    "    tide_data = pd.read_csv(fn_tides, parse_dates=['dates'])\n",
    "    dates_ts = [pytz.utc.localize(_.to_pydatetime()) for _ in tide_data['dates']]\n",
    "    tides_ts = np.array(tide_data['tides'])\n",
    "    # get tide levels corresponding to the time of image acquisition\n",
    "    dates_sat = output['dates']\n",
    "    tides_sat = SDS_tools.get_closest_datapoint(dates_sat, dates_ts, tides_ts)\n",
    "    # plot the subsampled tide data\n",
    "    fig, ax = plt.subplots(1,1,figsize=(15,4), tight_layout=True)\n",
    "    ax.grid(which='major', linestyle=':', color='0.5')\n",
    "    ax.plot(tide_data['dates'], tide_data['tides'], '-', color='0.6', label='all time-series')\n",
    "    ax.plot(dates_sat, tides_sat, '-o', color='k', ms=6, mfc='w',lw=1, label='image acquisition')\n",
    "    ax.set(ylabel='tide level [m]',xlim=[dates_sat[0],dates_sat[-1]], title='Water levels at the time of image acquisition')\n",
    "    ax.legend()\n",
    "    fig.savefig(os.path.join(inputs['filepath'], inputs['sitename'], 'tide_timeseries.jpg'),dpi=200) \n",
    "    # tidal correction along each transect\n",
    "    reference_elevation = sites_info[sitename]['contour_level'] # elevation at which you would like the shoreline time-series to be\n",
    "    beach_slope = sites_info[sitename]['beach_slope']\n",
    "    cross_distance_tidally_corrected, tidal_corrections = {},{}\n",
    "    for key in cross_distance.keys():\n",
    "        correction = (tides_sat-reference_elevation)/beach_slope\n",
    "        tidal_corrections[key] = correction\n",
    "        cross_distance_tidally_corrected[key] = cross_distance[key] + correction\n",
    "    # remove outliers\n",
    "    print('removing outliers...')\n",
    "    cross_distance_tidally_corrected = SDS_transects.reject_outliers(cross_distance_tidally_corrected,output,settings_transects)\n",
    "    # save in .csv files\n",
    "    fp_raw_timeseries = os.path.join(inputs['filepath'], inputs['sitename'], 'raw_timeseries')\n",
    "    fp_tc_timeseries = os.path.join(inputs['filepath'], inputs['sitename'], 'tidally_corected_timeseries')\n",
    "    if not os.path.exists(fp_raw_timeseries): os.makedirs(fp_raw_timeseries)\n",
    "    if not os.path.exists(fp_tc_timeseries): os.makedirs(fp_tc_timeseries)\n",
    "    for key in cross_distance_tidally_corrected.keys():\n",
    "        out_dict = dict([])\n",
    "        out_dict['dates'] = output['dates']\n",
    "        out_dict[key] = cross_distance_tidally_corrected[key]\n",
    "        out_dict['satname'] = output['satname']\n",
    "        df = pd.DataFrame(out_dict)\n",
    "        df.index=df['dates']\n",
    "        df.pop('dates')\n",
    "        # save tidally_corrected timeseries\n",
    "        fn = os.path.join(fp_tc_timeseries,'%s_timeseries_tidally_corrected.csv'%key)\n",
    "        df.to_csv(fn, sep=',')\n",
    "        # save raw timeseries\n",
    "        out_dict[key] = cross_distance_tidally_corrected[key] - tidal_corrections[key]\n",
    "        df = pd.DataFrame(out_dict)\n",
    "        df.index=df['dates']\n",
    "        df.pop('dates')\n",
    "        fn = os.path.join(fp_raw_timeseries,'%s_timeseries_raw.csv'%key)\n",
    "        df.to_csv(fn, sep=',')\n",
    "    # plot time-series\n",
    "    %matplotlib qt\n",
    "    fp_plots = os.path.join(inputs['filepath'], inputs['sitename'], 'plots')\n",
    "    if not os.path.exists(fp_plots): os.makedirs(fp_plots)\n",
    "    month_colors = plt.cm.get_cmap('tab20')\n",
    "    for key in cross_distance_tidally_corrected.keys():\n",
    "        chainage = cross_distance_tidally_corrected[key]\n",
    "        # remove nans\n",
    "        idx_nan = np.isnan(chainage)\n",
    "        dates_nonan = [dates_sat[_] for _ in np.where(~idx_nan)[0]]\n",
    "        chainage = chainage[~idx_nan] \n",
    "        # compute shoreline monthly averages\n",
    "        dict_month, dates_month, chainage_month, list_month = SDS_transects.monthly_average(dates_nonan, chainage)\n",
    "        # plot monthly averages\n",
    "        fig,ax=plt.subplots(1,1,figsize=[14,4],tight_layout=True)\n",
    "        ax.grid(b=True,which='major', linestyle=':', color='0.5')\n",
    "        ax.set_title('Time-series at %s'%key, x=0, ha='left')\n",
    "        ax.set(ylabel='distance [m]')\n",
    "        ax.plot(dates_nonan, chainage,'+', lw=1, color='k', mfc='w', ms=4, alpha=0.5,label='raw datapoints')\n",
    "        ax.plot(dates_month, chainage_month, '-', lw=2, color='k', mfc='w', ms=4, label='monthly-averaged')\n",
    "        # for k,month in enumerate(dict_month.keys()):\n",
    "        #     ax.plot(dict_month[month]['dates'], dict_month[month]['chainages'],\n",
    "        #              'o', mec='k', color=month_colors(k), label=month,ms=5)\n",
    "        ax.legend(loc='lower left',ncol=7,markerscale=1.5,frameon=True,edgecolor='k',columnspacing=1)\n",
    "        fig.savefig(os.path.join(fp_plots,'%s_timeseries.jpg'%key),dpi=200)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Comparison with groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for comparing and plotting the two time-series\n",
    "settings = {'min_days':  3,       # numbers of days difference under which to use nearest neighbour interpolation\n",
    "            'max_days':  10,      # maximum number of days difference to do a comparison\n",
    "            'binwidth':  3,       # binwidth for histogram plotting\n",
    "            'lims':      [-50,50] # cross-shore change limits for plotting purposes\n",
    "           }\n",
    "inputs = {}\n",
    "inputs['filepath'] =  os.path.join(os.getcwd(),'satellite_data')\n",
    "selected_transects = {\n",
    "    'NARRABEEN': ['PF1','PF2','PF4','PF6','PF8'],\n",
    "    'DUCK':      ['-91','1','1006','1097'],\n",
    "    'TRUCVERT':   ['-400','-300','-200','-100'],\n",
    "    'TORREYPINES':['PF525','PF530','PF535','PF540','PF545','PF570','PF575','PF580','PF585','PF593','PF598'],    \n",
    "    }\n",
    "for sitename in names_datasets:\n",
    "    print('Processing %s'%sitename)\n",
    "    inputs['sitename'] = sitename\n",
    "    fp_tc_timeseries = os.path.join(inputs['filepath'], inputs['sitename'], 'tidally_corected_timeseries')\n",
    "    # load survey data\n",
    "    data_folder = os.path.join(fp_datasets,sitename)\n",
    "    with open(os.path.join(data_folder, '%s_groundtruth.pkl'%sitename), 'rb') as f:\n",
    "        gt = pickle.load(f)\n",
    "    chain_sat_all = []\n",
    "    chain_sur_all = []\n",
    "    satnames_all = []\n",
    "    fp_comp = os.path.join(inputs['filepath'],inputs['sitename'],'comparison')\n",
    "    if not os.path.exists(fp_comp): os.makedirs(fp_comp)\n",
    "    for key in selected_transects[sitename]:\n",
    "        # load satellite time-series\n",
    "        fp_ts = os.path.join(fp_tc_timeseries,'%s_timeseries_tidally_corrected.csv'%key)\n",
    "        ts = pd.read_csv(fp_ts,parse_dates=['dates'])\n",
    "        ts_sat, ts_sur, satnames, fig = SDS_tools.compare_timeseries(ts,gt,key,settings)\n",
    "        fig.savefig(os.path.join(fp_comp,'%s_comparison_transect_%s.jpg'%(sitename,key)), dpi=150)\n",
    "        plt.close(fig)\n",
    "        chain_sat_all = np.append(chain_sat_all,ts_sat)\n",
    "        chain_sur_all = np.append(chain_sur_all,ts_sur)\n",
    "        satnames_all = satnames_all + satnames  \n",
    "    # calculate statistics for all transects together\n",
    "    chain_error = chain_sat_all - chain_sur_all\n",
    "    slope, intercept, rvalue, pvalue, std_err = stats.linregress(chain_sur_all, chain_sat_all)\n",
    "    R2 = rvalue**2\n",
    "    rmse = np.sqrt(np.mean((chain_error)**2))\n",
    "    mean = np.mean(chain_error)\n",
    "    std = np.std(chain_error)\n",
    "    q90 = np.percentile(np.abs(chain_error), 90)\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5), tight_layout=True)\n",
    "    # histogram\n",
    "    ax[0].grid(which='major',linestyle=':',color='0.5')\n",
    "    ax[0].axvline(x=0, ls='--', lw=1.5, color='k')\n",
    "    binwidth = settings['binwidth']\n",
    "    bins = np.arange(min(chain_error), max(chain_error) + binwidth, binwidth)\n",
    "    density = ax[0].hist(chain_error, bins=bins, density=True, color='0.6', edgecolor='k', alpha=0.5)\n",
    "    mu, std = stats.norm.fit(chain_error)\n",
    "    pval = stats.normaltest(chain_error)[1]\n",
    "    xlims = settings['lims']\n",
    "    x = np.linspace(xlims[0], xlims[1], 100)\n",
    "    p = stats.norm.pdf(x, mu, std)\n",
    "    ax[0].plot(x, p, 'r-', linewidth=1)\n",
    "    ax[0].set(xlabel='error [m]', ylabel='pdf', xlim=settings['lims'], title=sitename)\n",
    "    str_stats = ' rmse = %.1f\\n mean = %.1f\\n std = %.1f\\n q90 = %.1f' % (rmse, mean, std, q90)\n",
    "    ax[0].text(0, 0.98, str_stats,va='top', transform=ax[0].transAxes,fontsize=14)\n",
    "    # boxplot\n",
    "    data = []\n",
    "    median_data = []\n",
    "    n_data = []\n",
    "    ax[1].yaxis.grid()\n",
    "    for k,sat in enumerate(list(np.unique(satnames_all))):\n",
    "        idx = np.where([_ == sat for _ in satnames_all])[0]\n",
    "        data.append(chain_error[idx])\n",
    "        median_data.append(np.median(chain_error[idx]))\n",
    "        n_data.append(len(chain_error[idx]))\n",
    "    bp = ax[1].boxplot(data,0,'k.', labels=list(np.unique(satnames_all)), patch_artist=True)\n",
    "    for median in bp['medians']:\n",
    "        median.set(color='k', linewidth=1.5)\n",
    "    for j,boxes in enumerate(bp['boxes']):\n",
    "        boxes.set(facecolor='C'+str(j))\n",
    "        ax[1].text(j+1,median_data[j]+1, '%.1f' % median_data[j], horizontalalignment='center', fontsize=14)\n",
    "        ax[1].text(j+1+0.35,median_data[j]+1, ('n=%.d' % int(n_data[j])), ha='center', va='center', fontsize=12, rotation='vertical')\n",
    "    ax[1].set(ylabel='error [m]', ylim=settings['lims']);\n",
    "    fig.savefig(os.path.join(inputs['filepath'], inputs['sitename'],'%s_comparison_transect_all.jpg'%(sitename)), dpi=150)\n",
    "    fig.savefig(os.path.join(os.getcwd(),'%s_comparison_transect_all.jpg'%(sitename)), dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
